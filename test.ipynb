{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note that `empty_sequence` uses the KL divergence with Barnes-Hut approximation (angle=0.5) by default.\n",
      "Data loaded\n",
      "Please note that `empty_sequence` uses the KL divergence with Barnes-Hut approximation (angle=0.5) by default.\n",
      "Sequence defined\n",
      "[[-3.83978186e-05  6.36707264e-05]\n",
      " [ 2.12960411e-04  2.11776161e-04]\n",
      " [ 1.64758967e-04  1.70639410e-04]\n",
      " ...\n",
      " [ 1.09413224e-04  3.22986780e-05]\n",
      " [-3.89825000e-05 -2.25648346e-05]\n",
      " [-9.27048768e-06 -5.83760266e-05]]\n",
      "[[-3.83978186e-05  6.36707264e-05]\n",
      " [ 2.12960411e-04  2.11776161e-04]\n",
      " [ 1.64758967e-04  1.70639410e-04]\n",
      " ...\n",
      " [ 1.09413224e-04  3.22986780e-05]\n",
      " [-3.89825000e-05 -2.25648346e-05]\n",
      " [-9.27048768e-06 -5.83760266e-05]]\n",
      "[HyperbolicTSNE] Received iterable as input. It should have len=2 and contain (D=None, V=None)\n",
      "[hd_mat] Warning: There is nothing to do with given parameters. Returning given D and V\n",
      "Running Gradient Descent, Verbosity: True\n",
      "[gradient_descent] Warning!: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   0%|          | 1/250 [00:01<04:35,  1.10s/it]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 2/250 [00:01<02:09,  1.91it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 2/250 [00:01<02:09,  1.91it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   2%|▏         | 4/250 [00:01<01:01,  3.97it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   2%|▏         | 5/250 [00:01<00:51,  4.74it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   2%|▏         | 5/250 [00:01<00:51,  4.74it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   3%|▎         | 7/250 [00:01<00:37,  6.52it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   3%|▎         | 8/250 [00:01<00:35,  6.86it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   3%|▎         | 8/250 [00:01<00:35,  6.86it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   4%|▎         | 9/250 [00:02<00:53,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Running Gradient Descent, Verbosity: True\n",
      "[gradient_descent] Warning!: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent:   0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   0%|          | 0/750 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   0%|          | 2/750 [00:00<01:00, 12.33it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   0%|          | 2/750 [00:00<01:00, 12.33it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 4/750 [00:00<00:59, 12.46it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 4/750 [00:00<00:59, 12.46it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 6/750 [00:00<01:02, 11.99it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 6/750 [00:00<01:02, 11.99it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 8/750 [00:00<01:00, 12.20it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 8/750 [00:00<01:00, 12.20it/s]"
     ]
    },
    {
     "ename": "LaunchError",
     "evalue": "cuLaunchKernel failed: too many resources requested for launch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Git\\hyperbolic-tsne\\hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py:200\u001b[0m, in \u001b[0;36mhyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\u001b[1;34m()\u001b[0m\n\u001b[0;32m    196\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Call the CUDA kernel\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# You would need to modify this part to fit your actual CUDA kernel invocation\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m cuda_func(np\u001b[38;5;241m.\u001b[39mint32(start),\n\u001b[0;32m    201\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_samples),\n\u001b[0;32m    202\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(n_dimensions),\n\u001b[0;32m    203\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_size),\n\u001b[0;32m    204\u001b[0m           np\u001b[38;5;241m.\u001b[39mint32(grid_n),\n\u001b[0;32m    205\u001b[0m           pos_gpu,\n\u001b[0;32m    206\u001b[0m           negf_gpu,\n\u001b[0;32m    207\u001b[0m           grid_square_indices_per_point_gpu,\n\u001b[0;32m    208\u001b[0m           result_indices_gpu,\n\u001b[0;32m    209\u001b[0m           result_starts_counts_gpu,\n\u001b[0;32m    210\u001b[0m           max_distances_gpu,\n\u001b[0;32m    211\u001b[0m           square_positions_gpu,\n\u001b[0;32m    212\u001b[0m           sumQ_gpu,\n\u001b[0;32m    213\u001b[0m           block\u001b[38;5;241m=\u001b[39m(block_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), grid\u001b[38;5;241m=\u001b[39m(num_blocks, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    217\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py:502\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m    500\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_handlers \u001b[38;5;129;01mor\u001b[39;00m time_kernel:\n\u001b[0;32m    505\u001b[0m     Context\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mLaunchError\u001b[0m: cuLaunchKernel failed: too many resources requested for launch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_gpu'\n",
      "Traceback (most recent call last):\n",
      "  File \"hyperbolicTSNE\\hyperbolic_barnes_hut\\gpu.py\", line 200, in hyperbolicTSNE.hyperbolic_barnes_hut.tsne.uniform_grid_compute_gradient_negative_gpu\n",
      "    cuda_func(np.int32(start),\n",
      "  File \"c:\\Users\\Milan\\miniconda3\\envs\\htsne\\lib\\site-packages\\pycuda\\driver.py\", line 502, in function_call\n",
      "    func._launch_kernel(grid, block, arg_buf, shared, None)\n",
      "pycuda._driver.LaunchError: cuLaunchKernel failed: too many resources requested for launch\n",
      "Gradient Descent error: 0.00000 grad_norm: 0.00000e+00:   1%|          | 9/750 [00:00<01:10, 10.49it/s]\n",
      "C:\\Users\\Milan\\AppData\\Local\\Temp\\ipykernel_12024\\1048187734.py:135: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Execution time: 2.956752300262451 seconds\n",
      "Please note that `empty_sequence` uses the KL divergence with Barnes-Hut approximation (angle=0.5) by default.\n",
      "Sequence defined\n",
      "[HyperbolicTSNE] Received iterable as input. It should have len=2 and contain (D=None, V=None)\n",
      "[hd_mat] Warning: There is nothing to do with given parameters. Returning given D and V\n",
      "Running Gradient Descent, Verbosity: True\n",
      "[gradient_descent] Warning!: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent error: 93.97985 grad_norm: 8.32096e-01:  16%|█▌        | 39/250 [00:12<01:07,  3.13it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import time\n",
    "#from cuda import cuda, nvrtc\n",
    "\n",
    "from hyperbolicTSNE.util import find_last_embedding\n",
    "from hyperbolicTSNE.visualization import plot_poincare, animate\n",
    "from hyperbolicTSNE import load_data, Datasets, SequentialOptimizer, initialization, HyperbolicTSNE\n",
    "\n",
    "data_home = \"datasets\"\n",
    "log_path = \"temp/poincaregpu/\"  # path for saving embedding snapshots\n",
    "\n",
    "only_animate = False\n",
    "seed = 42\n",
    "dataset = Datasets.PLANARIA  # the Datasets handler provides access to several data sets used throughout the repository\n",
    "num_points = 20000  # we use a subset for demonstration purposes, full MNIST has N=70000\n",
    "perp = 30  # we use a perplexity of 30 in this example\n",
    "\n",
    "dataX, dataLabels, D, V, _ = load_data(\n",
    "    dataset, \n",
    "    data_home=data_home, \n",
    "    random_state=seed, \n",
    "    to_return=\"X_labels_D_V\",\n",
    "    hd_params={\"perplexity\": perp}, \n",
    "    sample=num_points, \n",
    "    knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    ")\n",
    "\n",
    "print(\"Data loaded\")\n",
    "\n",
    "exaggeration_factor = 12  # Just like regular t-SNE, we use early exaggeration with a factor of 12\n",
    "learning_rate = (dataX.shape[0] * 1) / (exaggeration_factor * 1000)  # We adjust the learning rate to the hyperbolic setting\n",
    "ex_iterations = 250  # The embedder is to execute 250 iterations of early exaggeration, ...\n",
    "main_iterations = 750  # ... followed by 750 iterations of non-exaggerated gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============= RUNNING EXACT GPU =============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "opt_config = dict(\n",
    "    learning_rate_ex=learning_rate,  # learning rate during exaggeration\n",
    "    learning_rate_main=learning_rate,  # learning rate main optimization \n",
    "    exaggeration=exaggeration_factor, \n",
    "    exaggeration_its=ex_iterations, \n",
    "    gradientDescent_its=main_iterations, \n",
    "    vanilla=False,  # if vanilla is set to true, regular gradient descent without any modifications is performed; for  vanilla set to false, the optimization makes use of momentum and gains\n",
    "    momentum_ex=0.5,  # Set momentum during early exaggeration to 0.5\n",
    "    momentum=0.8,  # Set momentum during non-exaggerated gradient descent to 0.8\n",
    "    exact=False,  # To use the quad tree for acceleration (like Barnes-Hut in the Euclidean setting) or to evaluate the gradient exactly\n",
    "    area_split=False,  # To build or not build the polar quad tree based on equal area splitting or - alternatively - on equal length splitting\n",
    "    n_iter_check=10,  # Needed for early stopping criterion\n",
    "    size_tol=0.999  # Size of the embedding to be used as early stopping criterion\n",
    ")\n",
    "\n",
    "opt_params = SequentialOptimizer.sequence_poincare(**opt_config)\n",
    "\n",
    "print(\"Sequence defined\")\n",
    "\n",
    "# Start: configure logging\n",
    "logging_dict = {\n",
    "    \"log_path\": log_path\n",
    "}\n",
    "opt_params[\"logging_dict\"] = logging_dict\n",
    "\n",
    "log_path = opt_params[\"logging_dict\"][\"log_path\"]\n",
    "# Delete old log path\n",
    "if os.path.exists(log_path) and not only_animate:\n",
    "    import shutil\n",
    "    shutil.rmtree(log_path)\n",
    "# End: logging\n",
    "\n",
    "# Compute an initial embedding of the data via PCA\n",
    "X_embedded = initialization(\n",
    "    n_samples=dataX.shape[0],\n",
    "    n_components=2,\n",
    "    X=dataX,\n",
    "    random_state=seed,\n",
    "    method=\"pca\"\n",
    ")\n",
    "print(X_embedded)\n",
    "\n",
    "# Initialize the embedder\n",
    "htsne = HyperbolicTSNE(\n",
    "    init=X_embedded, \n",
    "    n_components=2, \n",
    "    metric=\"precomputed\", \n",
    "    verbose=True, \n",
    "    opt_method=SequentialOptimizer, \n",
    "    opt_params=opt_params\n",
    ")\n",
    "\n",
    "X_embedded = initialization(\n",
    "    n_samples=dataX.shape[0],\n",
    "    n_components=2,\n",
    "    X=dataX,\n",
    "    random_state=seed,\n",
    "    method=\"pca\"\n",
    ")\n",
    "print(X_embedded)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "except ValueError:\n",
    "    print(\"Error!\")\n",
    "    hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "# Create a rendering of the embedding and save it to a file\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "fig = plot_poincare(hyperbolicEmbedding, dataLabels)\n",
    "fig.show()\n",
    "fig.savefig(f\"results/{dataset.name}-inexact.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============= RUNNING EXACT CPU =============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "opt_config = dict(\n",
    "    learning_rate_ex=learning_rate,  # learning rate during exaggeration\n",
    "    learning_rate_main=learning_rate,  # learning rate main optimization \n",
    "    exaggeration=exaggeration_factor, \n",
    "    exaggeration_its=ex_iterations, \n",
    "    gradientDescent_its=main_iterations, \n",
    "    vanilla=False,  # if vanilla is set to true, regular gradient descent without any modifications is performed; for  vanilla set to false, the optimization makes use of momentum and gains\n",
    "    momentum_ex=0.5,  # Set momentum during early exaggeration to 0.5\n",
    "    momentum=0.8,  # Set momentum during non-exaggerated gradient descent to 0.8\n",
    "    exact=True,  # To use the quad tree for acceleration (like Barnes-Hut in the Euclidean setting) or to evaluate the gradient exactly\n",
    "    area_split=False,  # To build or not build the polar quad tree based on equal area splitting or - alternatively - on equal length splitting\n",
    "    n_iter_check=10,  # Needed for early stopping criterion\n",
    "    size_tol=0.999  # Size of the embedding to be used as early stopping criterion\n",
    ")\n",
    "\n",
    "opt_params = SequentialOptimizer.sequence_poincare(**opt_config)\n",
    "\n",
    "print(\"Sequence defined\")\n",
    "\n",
    "log_path = \"temp/poincarecpu/\"  # path for saving embedding snapshots\n",
    "\n",
    "# Start: configure logging\n",
    "logging_dict = {\n",
    "    \"log_path\": log_path\n",
    "}\n",
    "opt_params[\"logging_dict\"] = logging_dict\n",
    "\n",
    "log_path = opt_params[\"logging_dict\"][\"log_path\"]\n",
    "# Delete old log path\n",
    "if os.path.exists(log_path) and not only_animate:\n",
    "    import shutil\n",
    "    shutil.rmtree(log_path)\n",
    "# End: logging\n",
    "\n",
    "# Compute an initial embedding of the data via PCA\n",
    "X_embedded = initialization(\n",
    "    n_samples=dataX.shape[0],\n",
    "    n_components=2,\n",
    "    X=dataX,\n",
    "    random_state=seed,\n",
    "    method=\"pca\"\n",
    ")\n",
    "\n",
    "# Initialize the embedder\n",
    "htsne = HyperbolicTSNE(\n",
    "    init=X_embedded, \n",
    "    n_components=2, \n",
    "    metric=\"precomputed\", \n",
    "    verbose=True, \n",
    "    opt_method=SequentialOptimizer, \n",
    "    opt_params=opt_params\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "except ValueError:\n",
    "    print(\"Error!\")\n",
    "    hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "    traceback.print_exc()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "# Create a rendering of the embedding and save it to a file\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "fig = plot_poincare(hyperbolicEmbedding, dataLabels)\n",
    "fig.show()\n",
    "fig.savefig(f\"results/{dataset.name}-exact.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htsne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
